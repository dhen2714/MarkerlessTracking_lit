@article{Geng2011,
abstract = {We provide a review of recent advances in 3D surface imaging technologies. We focus particularly on noncontact 3D surface measurement techniques based on structured illumination. The high-speed and high-resolution pattern projection capability offered by the digital light projection technology, together with the recent advances in imaging sensor technologies, may enable new generation systems for 3D surface measurement applications that will provide much better functionality and performance than existing ones in terms of speed, accuracy, resolution, modularization, and ease of use. Performance indexes of 3D imaging system are discussed, and various 3D surface imaging schemes are categorized, illustrated, and compared. Calibration techniques are also discussed, since they play critical roles in achieving the required precision. Numerous applications of 3D surface imaging technologies are discussed with several examples.},
author = {Geng, J},
doi = {10.1364/AOP.3.000128},
file = {:C$\backslash$:/Users/dhen2714/Documents/PHD/Experiments/Cameras/Datasheets/aop-3-2-128.pdf:pdf},
isbn = {1943-8206},
issn = {1943-8206},
journal = {Advances in Optics and Photonics},
number = {2},
pages = {128},
title = {{Structured-light 3D surface imaging: a tutorial}},
url = {https://www.osapublishing.org/abstract.cfm?URI=aop-3-2-128},
volume = {3},
year = {2011}
}

@article{JasonGeng1996,
	annote = {Cited By :102
	
	Export Date: 13 June 2017},
	author = {Geng, J},
	journal = {Optical Engineering},
	number = {2},
	pages = {376--383},
	title = {{Rainbow three-dimensional camera: New concept of high-speed three-dimensional vision systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007798308{\&}partnerID=40{\&}md5=0f466364be0b646957cca1b8952878cb},
	volume = {35},
	year = {1996}
}

@article{Salvi2004,
	abstract = {Coded structured light is considered one of the most reliable techniques for recovering the surface of objects. This technique is based on projecting a light pattern and viewing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be easily found. The decoded points can be triangulated and 3D information is obtained. We present an overview of the existing techniques, as well as a new and definitive classification of patterns for structured light sensors. We have implemented a set of representative techniques in this field and present some comparative results. The advantages and constraints of the different patterns are also discussed. ?? 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
	author = {Salvi, Joaquim and Pag\'{e}s, Jordi and Batlle, Joan},
	doi = {10.1016/j.patcog.2003.10.002},
	file = {:C$\backslash$:/Users/dhen2714/Downloads/1-s2.0-S0031320303003303-main.pdf:pdf},
	isbn = {0031-3203},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {3D measuring devices,Active stereo,Coded patterns,Computer vision,Structured light},
	number = {4},
	pages = {827--849},
	pmid = {3},
	title = {{Pattern codification strategies in structured light systems}},
	volume = {37},
	year = {2004}
}

@INPROCEEDINGS{Miyasaka00highspeed,
	author = {Takeo Miyasaka and Kazuhiro Kuroda and Makoto Hirose and Kazuo Araki},
	title = {High speed 3-D measurement system using incoherent light source for human performance analysis},
	booktitle = {in: Proceedings of the 19th Congress of The International Society for Photogrammetry and Remote Sensing, The},
	year = {2000},
	pages = {65--69}
}

@article{Carrihill1985,
	annote = {Cited By :99
	
	Export Date: 14 June 2017},
	author = {Carrihill, B and Hummel, R},
	journal = {Computer Vision, Graphics, {\&} Image Processing},
	number = {3},
	pages = {337--358},
	title = {{Experiments with the intensity ratio depth sensor.}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022267861{\&}partnerID=40{\&}md5=df0ddb80da52da16d11cfc751953430b},
	volume = {32},
	year = {1985}
}

@inproceedings{Tajima1990,
	annote = {Cited By :60
	
	Export Date: 14 June 2017},
	author = {Tajima, Johji and Iwakawa, Masato},
	booktitle = {Proceedings - International Conference on Pattern Recognition},
	pages = {309--313},
	title = {{3-D data acquisition by rainbow range finder}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025555449{\&}partnerID=40{\&}md5=b09d7cf90cfb5140b46d55e4450ece29},
	volume = {1},
	year = {1990}
}

@inproceedings{Fernandez2010,
	annote = {Export Date: 14 June 2017},
	author = {Fernandez, S and Salvi, J and Pribanic, T},
	booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
	doi = {10.1109/CVPRW.2010.5543483},
	pages = {64--71},
	title = {{Absolute phase mapping for one-shot dense pattern projection}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956538381{\&}doi=10.1109{\%}2FCVPRW.2010.5543483{\&}partnerID=40{\&}md5=fcd61e15bd8be1b7821bcda9fc78c0fa},
	year = {2010}
}

@article{Huang1999,
	abstract = {A color-encoded digital fringe projection technique is proposed for$\backslash$nhigh-speed 3-D surface contouring applications. In this technique,$\backslash$na color fringe pattern whose RGB components comprise three phase-shifted$\backslash$nfringe patterns is created by software on a computer screen and$\backslash$nthen projected to an object by a novel computer-controlled digital$\backslash$nprojection system. The image of the object is captured by a digital$\backslash$ncamera positioned at an angle different from that of the projection$\backslash$nsystem. The image is then separated into its RGB components, creating$\backslash$nthree phase-shifted images of the object. These three images are$\backslash$nused to retrieve the 3-D surface contour of the object through the$\backslash$nuse of a phase wrapping and unwrapping algorithm. Only one image$\backslash$nof the object is required to obtain the 3-D surface contour of the$\backslash$nobject. Thus contouring speed, limited only by the frame rate of$\backslash$nthe camera, can be dramatically increased as compared to that of$\backslash$nthe traditional phase-shifting techniques. The technique is especially$\backslash$nuseful in applications where the object being contoured is going$\backslash$nthrough quasi-static or dynamic changes. The principle of the technique$\backslash$nis described and some preliminary experimental results are presented.},
	author = {Huang, P.S. and Hu, Qingying and Jin, Feng and Chiang, Fu-Pen},
	doi = {10.1117/1.602151},
	file = {:C$\backslash$:/Users/dhen2714/Downloads/1065{\_}1.pdf:pdf},
	issn = {0091-3286},
	journal = {Optical Engineering},
	number = {6},
	pages = {1065},
	title = {{Color-encoded digital fringe projection technique for high-speed three-dimensional surface contouring}},
	url = {http://opticalengineering.spiedigitallibrary.org/article.aspx?articleid=1075760},
	volume = {38},
	year = {1999}
}

@article{Salvi2010,
	abstract = {Shape reconstruction using coded structured light is considered one of the most reliable techniques to recover object surfaces. Having a calibrated projector-camera pair, a light pattern is projected onto the scene and imaged by the camera. Correspondences between projected and recovered patterns are found and used to extract 3D surface information. This paper presents an up-to-date review and a new classification of the existing techniques. Some of these techniques have been implemented and compared, obtaining both qualitative and quantitative results. The advantages and drawbacks of the different patterns and their potentials are discussed. ?? 2010 Elsevier Ltd. All rights reserved.},
	author = {Salvi, Joaquim and Fernandez, Sergio and Pribanic, Tomislav and Llado, Xavier},
	doi = {10.1016/j.patcog.2010.03.004},
	file = {:C$\backslash$:/Users/dhen2714/Downloads/1-s2.0-S003132031000124X-main.pdf:pdf},
	isbn = {0031-3203},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {3D measuring devices,Active stereo,Coded patterns,Computer vision,Fourier transform profilometry,Pattern projection,Structured light},
	number = {8},
	pages = {2666--2680},
	publisher = {Elsevier},
	title = {{A state of the art in structured light patterns for surface profilometry}},
	url = {http://dx.doi.org/10.1016/j.patcog.2010.03.004},
	volume = {43},
	year = {2010}
}

@article{ ISI:A1974U657500060,
	Author = {Bruning, JH and Herriott, DR and Gallagher, JE and Rosenfeld, DP and
	White, AD and Brangaccio, DJ},
	Title = {{Digital wavefront measuring interferometer for testing optical surfaces and lenses}},
	Journal = {{Applied Optics}},
	Year = {{1974}},
	Volume = {{13}},
	Number = {{11}},
	Pages = {{2693-2703}},
	DOI = {{10.1364/AO.13.002693}},
	ISSN = {{1559-128X}},
	EISSN = {{2155-3165}},
	Unique-ID = {{ISI:A1974U657500060}},
}

@article{Takeda:83,
	author = {Mitsuo Takeda and Kazuhiro Mutoh},
	journal = {Appl. Opt.},
	keywords = {},
	number = {24},
	pages = {3977--3982},
	publisher = {OSA},
	title = {Fourier transform profilometry for the automatic measurement of 3-D object shapes},
	volume = {22},
	month = {Dec},
	year = {1983},
	url = {http://ao.osa.org/abstract.cfm?URI=ao-22-24-3977},
	doi = {10.1364/AO.22.003977},
	abstract = {A new computer-based technique for automatic 3-D shape measurement is proposed and verified by experiments. In contrast to the moire contouring technique, a grating pattern projected onto the object surface is Fourier-transformed and processed in its spatial frequency domain as well as in its space-signal domain. This technique has a much higher sensitivity than the conventional moire technique and is capable of fully automatic distinction between a depression and an elevation on the object surface. There is no requirement for assigning fringe orders and interpolating data in the regions between contour fringes. The technique is free from errors caused by spurious moire fringes generated by the higher harmonic components of the grating pattern.},
}

@ARTICLE{Yue20071170,
	author={Yue, H.-M. and Su, X.-Y. and Liu, Y.-Z.},
	title={Fourier transform profilometry based on composite structured light pattern},
	journal={Optics and Laser Technology},
	year={2007},
	volume={39},
	number={6},
	pages={1170-1175},
	doi={10.1016/j.optlastec.2006.08.014},
	url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847354311&doi=10.1016%2fj.optlastec.2006.08.014&partnerID=40&md5=1aa11f79311528ff87d2a2975b9efdb4},
	document_type={Article},
	source={Scopus},
}

@ARTICLE{Gdeisat2006482,
	author={Gdeisat, M.A. and Burton, D.R. and Lalor, M.J.},
	title={Eliminating the zero spectrum in Fourier transform profilometry using a two-dimensional continuous wavelet transform},
	journal={Optics Communications},
	year={2006},
	volume={266},
	number={2},
	pages={482-489},
	doi={10.1016/j.optcom.2006.05.070},
	note={cited By 58},
	url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749161907&doi=10.1016%2fj.optcom.2006.05.070&partnerID=40&md5=b43234ebe3b07eca074814cebbd48ec8},
	document_type={Article},
	source={Scopus},
}

@article{Posdamer1982,
	annote = {Cited By :218
	
	Export Date: 15 June 2017},
	author = {Posdamer, J L and Altschuler, M D},
	doi = {10.1016/0146-664X(82)90096-X},
	journal = {Computer Graphics and Image Processing},
	number = {1},
	pages = {1--17},
	title = {{Surface measurement by space-encoded projected beam systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019931915{\&}doi=10.1016{\%}2F0146-664X{\%}2882{\%}2990096-X{\&}partnerID=40{\&}md5=0bd36bb42a486638349a8bb550aae514},
	volume = {18},
	year = {1982}
}

@article{Caspi1998,
	annote = {Cited By :238
	
	Export Date: 15 June 2017},
	author = {Caspi, D and Kiryati, N and Shamir, J},
	doi = {10.1109/34.682177},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	number = {5},
	pages = {470--480},
	title = {{Range imaging with adaptive color structured light}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032069591{\&}doi=10.1109{\%}2F34.682177{\&}partnerID=40{\&}md5=2776ef4134784bac043a450c91a6845d},
	volume = {20},
	year = {1998}
}

@article{Huang2006,
	annote = {Cited By :105
	
	Export Date: 15 June 2017},
	author = {Huang, P.S. and Zhang, S},
	doi = {10.1364/AO.45.005086},
	journal = {Applied Optics},
	number = {21},
	pages = {5086--5091},
	title = {{Fast three-step phase-shifting algorithm}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645516896{\&}doi=10.1364{\%}2FAO.45.005086{\&}partnerID=40{\&}md5=7e12f5bdf4957712b4d5021cd301dae5},
	volume = {45},
	year = {2006}
}

@article{Olesen2012,
	abstract = {We present a system for head motion tracking in 3D brain imaging. The system is based on facial surface reconstruction and tracking using a structured light (SL) scanning principle. The system is designed to fit into narrow 3D medical scanner geometries limiting the field of view. It is tested in a clinical setting on the high resolution research tomograph (HRRT), Siemens PET scanner with a head phantom and volunteers. The SL system is compared to a commercial optical tracking system, the Polaris Vicra system, from NDI based on translatory and rotary ground truth motions of the head phantom. The accuracy of the systems was similar, with root mean square (rms) errors of 0.09 degrees for ±20 degrees axial rotations, and rms errors of 0.24 mm for ± 25 mm translations. Tests were made using (1) a light emitting diode (LED) based miniaturized video projector, the Pico projector from Texas Instruments, and (2) a customized version of this projector replacing a visible light LED with a 850 nm near infrared LED. The latter system does not provide additional discomfort by visible light projection into the patient's eyes. The main advantage over existing head motion tracking devices, including the Polaris Vicra system, is that it is not necessary to place markers on the patient. This provides a simpler workflow and eliminates uncertainties related to marker attachment and stability. We show proof of concept of a marker less tracking system especially designed for clinical use with promising results.},
	author = {Olesen, Oline Vinter and Paulsen, Rasmus R. and H{\o}jgaard, Liselotte and Roed, Bjarne and Larsen, Rasmus},
	doi = {10.1109/TMI.2011.2165157},
	file = {:C$\backslash$:/Users/dhen2714/Documents/PHD/Experiments/Cameras/Datasheets/05986716.pdf:pdf},
	isbn = {1558-254X (Electronic)$\backslash$r0278-0062 (Linking)},
	issn = {02780062},
	journal = {IEEE Transactions on Medical Imaging},
	keywords = {Motion estimation,positron emission tomography,stereo image processing,stereo vision,structured light system},
	number = {1},
	pages = {79--87},
	pmid = {21859614},
	title = {{Motion tracking for medical imaging: A nonvisible structured light tracking approach}},
	volume = {31},
	year = {2012}
}

@article{Miranda2017,
	annote = {- 3D Point Cloud Library - check this out.},
	author = {Miranda, Alan and Staelens, Steven and Stroobants, Sigrid and Verhaeghe, Jeroen},
	doi = {10.1088/1361-6560/aa5a46},
	file = {:C$\backslash$:/Users/dhen2714/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miranda et al. - 2017 - Markerless rat head motion tracking using structured light for brain PET imaging of unrestrained awake small ani.pdf:pdf},
	issn = {0031-9155},
	journal = {Physics in Medicine and Biology},
	number = {5},
	pages = {1744--1758},
	publisher = {IOP Publishing},
	title = {{Markerless rat head motion tracking using structured light for brain PET imaging of unrestrained awake small animals}},
	url = {http://stacks.iop.org/0031-9155/62/i=5/a=1744?key=crossref.75b239a1104eef29358089eca27a5911},
	volume = {62},
	year = {2017}
}

@inproceedings{Olesen2015a,
	author = {Olesen, Oline Vinter and Wilm, Jakob and {Van der Kouwe}, Andre and Jensen, Rasmus Ramsb{\o}l and Larsen, Rasmus and Wald, Lawrence L},
	title = {{An MRI Compatible Surface Scanner}},
	year = {2015}
}

@article{Boyer1987,
	annote = {Cited By :228
	
	Export Date: 15 June 2017},
	author = {Boyer, K L and Kak, A C},
	doi = {10.1109/TPAMI.1987.4767869},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	number = {1},
	pages = {14--28},
	title = {{Color-Encoded Structured Light for Rapid Active Ranging}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023090022{\&}doi=10.1109{\%}2FTPAMI.1987.4767869{\&}partnerID=40{\&}md5=400550b9fdb9ac57137481efa7957dce},
	volume = {PAMI-9},
	year = {1987}
}

@inproceedings{Forster2007,
	annote = {Cited By :28
	
	Export Date: 15 June 2017},
	author = {Forster, F},
	booktitle = {Proceedings - Third International Symposium on 3D Data Processing, Visualization, and Transmission, 3DPVT 2006},
	doi = {10.1109/3DPVT.2006.13},
	pages = {208--215},
	title = {{A high-resolution and high accuracy real-time 3D sensor based on structured light}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249097131{\&}doi=10.1109{\%}2F3DPVT.2006.13{\&}partnerID=40{\&}md5=dd5b468f4bb4d704483ef3ee7f37adb8},
	year = {2007}
}

@inproceedings{Zhang2002,
	annote = {Cited By :337
	
	Export Date: 15 June 2017},
	author = {Zhang, L and Curless, B and Seitz, S M},
	booktitle = {Proceedings - 1st International Symposium on 3D Data Processing Visualization and Transmission, 3DPVT 2002},
	doi = {10.1109/TDPVT.2002.1024035},
	pages = {24--36},
	title = {{Rapid shape acquisition using color structured light and multi-pass dynamic programming}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964467108{\&}doi=10.1109{\%}2FTDPVT.2002.1024035{\&}partnerID=40{\&}md5=7606dad01f07b7bf4e5e77a75218ad66},
	year = {2002}
}

@inproceedings{Petriu2000,
	annote = {Cited By :34
	
	Export Date: 15 June 2017},
	author = {Petriu, E M and Sakr, Z and Spoelder, H J W and Moica, A},
	booktitle = {Conference Record - IEEE Instrumentation and Measurement Technology Conference},
	pages = {1237--1241},
	title = {{Object recognition using pseudo-random color encoded structured light}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033687492{\&}partnerID=40{\&}md5=dbbcc341229cacbec8d98baf83982837},
	volume = {3},
	year = {2000}
}

@inproceedings{Pages2006,
	annote = {Cited By :20
	
	Export Date: 15 June 2017},
	author = {Pag{\`{e}}s, J and Collewet, C and Chaumette, F and Salvi, J},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ROBOT.2006.1642335},
	pages = {4118--4123},
	title = {{An approach to visual servoing based on coded light}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845644712{\&}doi=10.1109{\%}2FROBOT.2006.1642335{\&}partnerID=40{\&}md5=1509ae50a2645f0a11c69a3995de5a2e},
	volume = {2006},
	year = {2006}
}

@inproceedings{Albitar2006,
	annote = {Cited By :18
	
	Export Date: 15 June 2017},
	author = {Albitar, C and Graebling, P and Doignon, C},
	booktitle = {Proceedings - International Conference on Image Processing, ICIP},
	doi = {10.1109/ICIP.2007.4379638},
	pages = {VI529--VI532},
	title = {{Design of a monochromatic pattern for a robust structured light coding}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-48149113292{\&}doi=10.1109{\%}2FICIP.2007.4379638{\&}partnerID=40{\&}md5=7a880a7c913627bbaa5adb88a9c4e8ab},
	volume = {6},
	year = {2006}
}

@article{Murphy-Chutorian2009,
	abstract = {The capacity to estimate the head pose of another person is a common human ability that presents a unique challenge for computer vision systems. Compared to face detection and recognition, which have been the primary foci of face-related vision research, identity-invariant head pose estimation has fewer rigorously evaluated systems or generic solutions. In this paper, we discuss the inherent difficulties in head pose estimation and present an organized survey describing the evolution of the field. Our discussion focuses on the advantages and disadvantages of each approach and spans 90 of the most innovative and characteristic papers that have been published on this topic. We compare these systems by focusing on their ability to estimate coarse and fine head pose, highlighting approaches that are well suited for unconstrained environments.},
	author = {Murphy-Chutorian, Erik and Trivedi, Mohan Manubhai},
	doi = {10.1109/TPAMI.2008.106},
	file = {:C$\backslash$:/Users/dhen2714/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy-Chutorian, Trivedi - 2009 - Head pose estimation in computer vision A survey.pdf:pdf},
	isbn = {0162-8828},
	issn = {01628828},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Face analysis,Facial landmarks,Gesture analysis,Head pose estimation,Human-computer interfaces},
	number = {4},
	pages = {607--626},
	pmid = {19229078},
	title = {{Head pose estimation in computer vision: A survey}},
	volume = {31},
	year = {2009}
}

@article{Lepetit2005,
	abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Aug- mented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated inter- faces can use. Computer Vision offers solutions that are cheap, practical and non-invasive. This survey reviews the different techniques and approaches that have been developed by industry and research. First, important math- ematical tools are introduced: Camera representation, robust estima- tion and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery con- cludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking. Because it encompasses many computer vision techniques from low- level vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system.},
	author = {Lepetit, Vincent and Fua, Pascal},
	doi = {10.1561/0600000001},
	file = {:C$\backslash$:/Users/dhen2714/Downloads/lepetit{\_}ftcgv05.pdf:pdf},
	isbn = {1933019034},
	issn = {1572-2740},
	journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
	number = {1},
	pages = {1--89},
	title = {{Monocular Model-Based 3D Tracking of Rigid Objects: A Survey}},
	url = {http://www.nowpublishers.com/article/Details/CGV-001},
	volume = {1},
	year = {2005}
}

@inproceedings{Ravela1995,
	annote = {Cited By :15
	
	Export Date: 22 June 2017},
	author = {Ravela, S and Draper, B and Lim, J and Weiss, R},
	booktitle = {IEEE International Conference on Intelligent Robots and Systems},
	pages = {174--180},
	title = {{Adaptive tracking and model registration across distinct aspects}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029190220{\&}partnerID=40{\&}md5=c2dde2d553c73972363a623690fca402},
	volume = {1},
	year = {1995}
}

@inproceedings{Anishchenko2015,
	author = {Anishchenko, Sergey and Beylin, David and Stepanov, Pavel and Stepanov, Alex and Weinberg, Irving N and Schaeffer, Stephen and Zavarzin, Valery and Shaposhnikov, Dmitry and Smith, Mark F},
	booktitle = {Proc. of IEEE Nuclear Science Symposium and Medical Imaging Conference},
	file = {:C$\backslash$:/Users/dhen2714/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anishchenko et al. - 2015 - Markerless Head Tracking Evaluation with Human Subjects for a Dedicated Brain PET Scanner.pdf:pdf},
	isbn = {9781467398626},
	number = {9},
	pages = {9862},
	title = {{Markerless Head Tracking Evaluation with Human Subjects for a Dedicated Brain PET Scanner}},
	volume = {35},
	year = {2015}
}

@article{Gao2007,
	author = {Gao, Xiaohong W and Anishenko, Sergey and Shaposhnikov, Dmitry and Podlachikova, Lubov and Batty, Stephen and Clark, John},
	file = {:C$\backslash$:/Users/dhen2714/Downloads/jcs2007.pdf:pdf},
	journal = {Journal of Computer Science},
	keywords = {degrading effects of motion,face segmentation,facial landmarks detection,head motion detection,human vision,models,pet,skin colour,these methods fall into},
	number = {7},
	pages = {528--532},
	title = {{High-precision Detection of Facial Landmarks to Estimate Head Motions Based on Vision Models}},
	volume = {3},
	year = {2007}
}

@inproceedings{Ma2008,
	author = {Ma, WPT and Hamarneh, G and Mori, Greg and Dinelle, Katie and Sossi, Vesna},
	booktitle = {IEEE Nuclear Science Symposium and Medical Imaging Conference},
	doi = {10.1109/NSSMIC.2008.4774180},
	file = {:C$\backslash$:/Users/dhen2714/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma et al. - 2008 - Motion estimation for functional medical imaging studies using a stereo video head pose tracking system.pdf:pdf},
	pages = {4086--4090},
	title = {{Motion estimation for functional medical imaging studies using a stereo video head pose tracking system}},
	url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4774180},
	year = {2008}
}

@article{Lowe2004,
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	author = {Lowe, David G},
	file = {:C$\backslash$:/Users/dhen2714/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive image features from scale invariant keypoints(2).pdf:pdf},
	isbn = {1568811012},
	issn = {0920-5691},
	journal = {Int'l Journal of Computer Vision},
	pages = {91--11020042},
	title = {{Distinctive image features from scale invariant keypoints}},
	volume = {60},
	year = {2004}
}

@article{Bay2008,
	abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. ?? 2007 Elsevier Inc. All rights reserved.},
	author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
	doi = {10.1016/j.cviu.2007.09.014},
	file = {:C$\backslash$:/Users/dhen2714/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bay et al. - 2008 - Speeded-Up Robust Features (SURF).pdf:pdf},
	isbn = {9783540338321},
	issn = {10773142},
	journal = {Computer Vision and Image Understanding},
	keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
	number = {3},
	pages = {346--359},
	pmid = {16081019},
	title = {{Speeded-Up Robust Features (SURF)}},
	volume = {110},
	year = {2008}
}

@article{Kyme2014,
	abstract = {Noninvasive functional imaging of awake, unrestrained small animals using motion-compensation removes the need for anesthetics and enables an animal's behavioral response to stimuli or administered drugs to be studied concurrently with imaging. While the feasibility of motion-compensated radiotracer imaging of awake rodents using marker-based optical motion tracking has been shown, markerless motion tracking would avoid the risk of marker detachment, streamline the experimental workflow, and potentially provide more accurate pose estimates over a greater range of motion. We have developed a stereoscopic tracking system which relies on native features on the head to estimate motion. Features are detected and matched across multiple camera views to accumulate a database of head landmarks and pose is estimated based on 3D-2D registration of the landmarks to features in each image. Pose estimates of a taxidermal rat head phantom undergoing realistic rat head motion via robot control had a root mean square error of 0.15 and 1.8 mm using markerless and marker-based motion tracking, respectively. Markerless motion tracking also led to an appreciable reduction in motion artifacts in motion-compensated positron emission tomography imaging of a live, unanesthetized rat. The results suggest that further improvements in live subjects are likely if nonrigid features are discriminated robustly and excluded from the pose estimation process.},
	author = {Kyme, A and Se, Stephen and Meikle, Steven and Angelis, Georgios and Ryder, Will and Popovic, Kata and Yatigammana, Dylan and Fulton, Roger},
	doi = {10.1109/TMI.2014.2332821},
	file = {:C$\backslash$:/Users/New/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kyme et al. - 2014 - Markerless motion tracking of awake animals in positron emission tomography.pdf:pdf},
	isbn = {1558-254X (Electronic)$\backslash$r0278-0062 (Linking)},
	issn = {1558254X},
	journal = {IEEE Transactions on Medical Imaging},
	keywords = {Markerless optical motion tracking,motion compensation,positron emission tomography (PET)},
	number = {11},
	pages = {2180--2190},
	pmid = {24988591},
	title = {{Markerless motion tracking of awake animals in positron emission tomography}},
	volume = {33},
	year = {2014}
}

@article{Se2002,
	author = {Se, Stephen and Lowe, David G and Little, Jim},
	file = {:C$\backslash$:/Users/New/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Se, Lowe, Little - 2002 - Mobile Robot Localization and Mapping with Uncertainty using Scale-Invariant Visual Landmarks.pdf:pdf},
	journal = {International Journal of Robotics Research},
	keywords = {localization,mapping,visual landmarks},
	number = {8},
	pages = {735--758},
	title = {{Mobile Robot Localization and Mapping with Uncertainty using Scale-Invariant Visual Landmarks}},
	volume = {21},
	year = {2002}
}

@article{Se2005,
	abstract = {We have previously developed a mobile robot system which uses scale-invariant visual landmarks to localize and simultaneously build three-dimensional (3-D) maps of unmodified environments. In this paper, we examine global localization, where the robot localizes itself globally, without any prior location estimate. This is achieved by matching distinctive visual landmarks in the current frame to a database map. A Hough transform approach and a RANSAC approach for global localization are compared, showing that RANSAC is much more efficient for matching specific features, but much worse for matching nonspecific features. Moreover, robust global localization can be achieved by matching a small submap of the local region built from multiple frames. This submap alignment algorithm for global localization can be applied to map building, which can be regarded as alignment of multiple 3-D submaps. A global minimization procedure is carried out using the loop closure constraint to avoid the effects of slippage and drift accumulation. Landmark uncertainty is taken into account in the submap alignment and the global minimization process. Experiments show that global localization can be achieved accurately using the scale-invariant landmarks. Our approach of pairwise submap alignment with backward correction in a consistent manner produces a better global 3-D map. 2005 IEEE.},
	author = {Se, Stephen and Lowe, David G. and Little, James J.},
	doi = {10.1109/TRO.2004.839228},
	file = {:C$\backslash$:/Users/New/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Se, Lowe, Little - 2005 - Vision-based global localization and mapping for mobile robots.pdf:pdf},
	isbn = {1552-3098},
	issn = {15523098},
	journal = {IEEE Transactions on Robotics},
	keywords = {Global localization,Map building,Mobile robots,Visual landmarks},
	number = {3},
	pages = {364--375},
	title = {{Vision-based global localization and mapping for mobile robots}},
	volume = {21},
	year = {2005}
}

@inproceedings{Jourabloo2016,
	annote = {Export Date: 25 June 2017},
	author = {Jourabloo, A and Liu, X},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
	doi = {10.1109/ICCV.2015.421},
	file = {:D$\backslash$:/Downloads/07410778.pdf:pdf},
	pages = {3694--3702},
	title = {{Pose-invariant 3D face alignment}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973856188{\&}doi=10.1109{\%}2FICCV.2015.421{\&}partnerID=40{\&}md5=ca38682e14abd0277b6f0aff46fefd27},
	volume = {11-18-Dece},
	year = {2016}
}

@inproceedings{Zhu2016,
	annote = {Export Date: 25 June 2017},
	author = {Zhu, X and Lei, Z and Liu, X and Shi, H and Li, S Z},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	file = {:D$\backslash$:/Downloads/07780392.pdf:pdf},
	pages = {146--155},
	title = {{Face alignment across large poses: A 3D solution}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986281478{\&}partnerID=40{\&}md5=4188ca4b3e9cc379fe2185a3eb8b24fd},
	volume = {2016-Janua},
	year = {2016}
}

@inproceedings{Gu2006,
	author = {Gu, Lie and Kanade, T},
	booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
	doi = {10.1109/CVPR.2006.11},
	file = {:D$\backslash$:/Downloads/01640900.pdf:pdf},
	isbn = {1063-6919 VO - 1},
	keywords = {Computer science,Computer vision,Deformable models,Image reconstruction,Iterative algorithms,Labeling,Lighting,Noise shaping,Shape,Surface fitting},
	pages = {1305--1312},
	title = {{3D Alignment of Face in a Single Image}},
	volume = {1},
	year = {2006}
}

@article{Dong2016,
	author = {Dong, Yanchao and Wang, Yanming and Yue, Jiguang and Hu, Zhencheng},
	doi = {10.3390/s16081157},
	file = {:D$\backslash$:/Downloads/sensors-16-01157.pdf:pdf},
	issn = {14248220},
	journal = {Sensors (Switzerland)},
	keywords = {3D facial movement,Eyelid,Facial animation,Facial feature points,HCI},
	number = {8},
	title = {{Real time 3D facial movement tracking using a monocular camera}},
	volume = {16},
	year = {2016}
}

@article{Jeni2017a,
	abstract = {To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60??. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of landmarks and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction, extension to multi-view reconstruction, temporal integration for videos and 3D head-pose estimation. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org.},
	annote = {From Duplicate 2 (Dense 3D face alignment from 2D video for real-time use - Jeni, L A; Cohn, J F; Kanade, T)
	
	Export Date: 25 June 2017},
	author = {Jeni, L A. and Cohn, J F. and Kanade, T},
	doi = {10.1016/j.imavis.2016.05.009},
	file = {:C$\backslash$:/Users/New/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jeni, Cohn, Kanade - 2017 - Dense 3D face alignment from 2D video for real-time use.pdf:pdf},
	isbn = {9781479960262},
	issn = {02628856},
	journal = {Image and Vision Computing},
	keywords = {3D face alignment,Dense 3D model,Real-time method},
	pages = {13--24},
	publisher = {Elsevier B.V.},
	title = {{Dense 3D face alignment from 2D video for real-time use}},
	url = {http://dx.doi.org/10.1016/j.imavis.2016.05.009 https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006747280{\&}doi=10.1016{\%}2Fj.imavis.2016.05.009{\&}partnerID=40{\&}md5=e04eee2e6c2caac1063ee1fce1853635},
	volume = {58},
	year = {2017}
}

@inproceedings{Kyme2016,
	author = {Kyme, A and Maclaren, Julian and Aksoy, Murat and Bammer, Roland},
	booktitle = {IEEE Nuclear Science Symposium, Medical Imaging Conference},
	title = {{Feasibility of Marker-Free Motion Tracking for Motion-Corrected MRI and PET-MRI}},
	year = {2016}
}

@inproceedings{Kyme2013,
	author = {Kyme, A and Se, Stephen and Meikle, Steven and Fulton, Roger},
	booktitle = {IEEE Nuclear Science Symposium Conference Record},
	doi = {10.1109/NSSMIC.2013.6829065},
	file = {:C$\backslash$:/Users/New/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kyme et al. - 2013 - Markerless motion tracking for motion-compensated clinical imaging.pdf:pdf},
	isbn = {9781479905348},
	issn = {10957863},
	pages = {2--5},
	title = {{Markerless motion tracking for motion-compensated clinical imaging}},
	year = {2013}
}

@book{tofbook,	
	author={Hansard,Miles and Lee,Seungkyu and Choi,Ouk and Horaud,Radu},	
	year={2013},	
	title={Time-of-Flight Cameras: Principles, Methods and Applications},	
	publisher={Springer London},	
	address={London},	
	keywords={Image processing; Computer science},	
	isbn={9781447146582;1447146581;},	
	language={English},	
}

@article{Horaud2016,
	abstract = {Time-of-flight (TOF) cameras are sensors that can measure the depths of scene-points, by illuminating the scene with a controlled laser or LED source, and then analyzing the reflected light. In this paper we will first describe the underlying measurement principles of time-of-flight cameras, including: (i) pulsed-light cameras, which measure directly the time taken for a light pulse to travel from the device to the object and back again, and (ii) continuous-wave modulated-light cameras, which measure the phase difference between the emitted and received signals, and hence obtain the travel time indirectly. We review the main existing designs, including prototypes as well as commercially available devices. We also review the relevant camera calibration principles, and how they are applied to TOF devices. Finally, we discuss the benefits and challenges of combined TOF and color camera systems. Keywords LIDAR {\^{A}}{\textperiodcentered} range scanners {\^{A}}{\textperiodcentered} single photon avalanche diode {\^{A}}{\textperiodcentered} time-of-flight cameras {\^{A}}{\textperiodcentered} 3D computer vision {\^{A}}{\textperiodcentered} active-light sensors},
	author = {Horaud, Radu and Hansard, Miles and Evangelidis, Georgios and M\'{e}nier, Cl\'{e}ment},
	doi = {10.1007/s00138-016-0784-4},
	file = {:C$\backslash$:/Users/dhen2714/Downloads/10.1007{\%}2Fs00138-016-0784-4.pdf:pdf},
	issn = {14321769},
	journal = {Machine Vision and Applications},
	keywords = {3D computer vision,Active light sensors,LIDAR,Range scanners,Single-photon avalanche diode,Time-of-flight cameras},
	number = {7},
	pages = {1005--1020},
	publisher = {Springer Berlin Heidelberg},
	title = {{An overview of depth cameras and range scanners based on time-of-flight technologies}},
	volume = {27},
	year = {2016}
}

